# 1. 什么是强化学习？

我们经常会思考人类是如何学习的。比如一个刚刚出生的婴儿，通过用手
去触摸物体，或者用眼睛看整个世界，在这个过程中并没有人指导他，
这时学习的方法就是通过与环境进行交互。因此从与环境的交互过程中
学习是一种显而易见的人类学习的方法。甚至可以认为，**从交互中学习
是几乎任何学习理论(Learning Theory)和智能理论(Intelligence Theory)的基本观点。**

强化学习(Reinforcement Learning)就是考虑怎么做才可以最大化奖励(Reward)，这个奖励是需要用数值来衡量的。换句话说，就是需要知道在什么状态下我们应该执行什么动作。
人们并不是直接知道在哪种状态下应该执行哪种动作，而是通过尝试，找到一个可以得到最大奖励的动作。一般情况下，选择了一个动作之后不仅会决定
我们当下会得到的奖励(Immediate Reward)，还会决定我们会进入哪一个新的状态，进而影响我们后续所有应该得到的奖赏，这个叫做
延迟奖励(Delayed Reward)。因此，**试错(Trial-and-error)和延迟奖励(Delayed Reward)是强化学习的两个最重要特征**

在强化学习中，一个智能体(Agent)--可以理解成一个人或者机器--必须可以感知环境并且能够通过一些动作改变状态。同时，智能体需要有目标，就是
明确自己的目的是什么。马尔科夫决策过程(Markov Decision Processes)包括上面三点要点（包括感知、动作、目标）。任何能解决这个问题的
方法都可以认为是一个强化学习方法。

# 2. 强化学习是一种机器学习范式

有人说机器学习分为监督学习和无监督学习，并且把强化学习当作是一种无监督学习问题。但是实际他们是完全不同的。
下面介绍下强化学习、监督学习、无监督学习的区别所在。

首先，强化学习不同于监督学习。监督学习相当于对于每一个状态都能知道最优动作是什么（即label），他的目标是探索那些没在训练集中出现
的样本的正确动作。这确实是一种非常重要的学习方式，但这不能够从交互中学习。在交互问题中，一般很难得到每种状态下又准又好的明确行为。
因此Agent通常是通过自己的经验来学习如何得到更大奖励的。

其次，强化学习不同于无监督学习。无监督学习的目的是找到无标签数据中隐藏的结构，比如社团发现等问题，而强化学习目的是最大化奖励，
不是找到数据的隐含结构信息。虽然结构信息可能对强化学习有帮助，但是这不足以使Agent得到最大奖赏。

因此，强化学习应该看作是与监督学习、无监督学习并列的一种机器学习范式(Paradigm)。

同时，在强化学习中会遇到一个挑战：如何平衡探索与利用(Trade-off between exploration and exploitation)，这是在监督学习和无监督学习中
不存在的一个挑战。这也称为探索-利用困境(Exploration-exploitation dilemma)。其实探索-利用困境意思就是我们既想利用我们以前找到过的比较
优秀的动作（就是可以得到更好的即时奖赏），又想尝试没有尝试过的动作去探索更好的动作。
